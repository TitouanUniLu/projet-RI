{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & representation learning: Neural Embeddings, Text Classification\n",
    "\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **Bag of Word (BoW)** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## [Dataset](https://thome.isir.upmc.fr/classes/RITAL/json_pol.json)\n",
    "\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (see next)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Loading json\n",
    "file = 'json_pol.json'\n",
    "with open(file,encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter = Counter((x[1] for x in data))\n",
    "print(\"Number of reviews : \", len(data))\n",
    "print(\"----> # of positive : \", counter[1])\n",
    "print(\"----> # of negative : \", counter[0])\n",
    "print(\"\")\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: train a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gensim not installed yet\n",
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 00:14:53,419 : INFO : collecting all words and their counts\n",
      "2024-02-08 00:14:53,419 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-08 00:14:54,141 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
      "2024-02-08 00:14:54,887 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
      "2024-02-08 00:14:55,225 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
      "2024-02-08 00:14:55,225 : INFO : Creating a fresh vocabulary\n",
      "2024-02-08 00:14:55,444 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-08T00:14:55.444444', 'gensim': '4.3.2', 'python': '3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-08 00:14:55,444 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-08T00:14:55.444444', 'gensim': '4.3.2', 'python': '3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-08 00:14:55,699 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2024-02-08 00:14:55,699 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-08 00:14:55,699 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-08T00:14:55.699596', 'gensim': '4.3.2', 'python': '3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-08 00:14:56,077 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
      "2024-02-08 00:14:56,077 : INFO : resetting layer weights\n",
      "2024-02-08 00:14:56,092 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-08T00:14:56.092757', 'gensim': '4.3.2', 'python': '3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\n",
      "2024-02-08 00:14:56,092 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-08T00:14:56.092757', 'gensim': '4.3.2', 'python': '3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-02-08 00:14:57,110 : INFO : EPOCH 0 - PROGRESS: at 9.49% examples, 399055 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:14:58,114 : INFO : EPOCH 0 - PROGRESS: at 18.19% examples, 374298 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:14:59,165 : INFO : EPOCH 0 - PROGRESS: at 26.39% examples, 359121 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:00,169 : INFO : EPOCH 0 - PROGRESS: at 35.24% examples, 361724 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:01,172 : INFO : EPOCH 0 - PROGRESS: at 45.38% examples, 372961 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:02,191 : INFO : EPOCH 0 - PROGRESS: at 54.22% examples, 369052 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:03,227 : INFO : EPOCH 0 - PROGRESS: at 63.35% examples, 370399 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:04,231 : INFO : EPOCH 0 - PROGRESS: at 73.45% examples, 375966 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:05,233 : INFO : EPOCH 0 - PROGRESS: at 81.30% examples, 369111 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:06,245 : INFO : EPOCH 0 - PROGRESS: at 90.44% examples, 370230 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:07,234 : INFO : EPOCH 0: training on 5713167 raw words (4165409 effective words) took 11.1s, 374341 effective words/s\n",
      "2024-02-08 00:15:08,268 : INFO : EPOCH 1 - PROGRESS: at 8.87% examples, 360539 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:09,270 : INFO : EPOCH 1 - PROGRESS: at 17.21% examples, 350928 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:10,278 : INFO : EPOCH 1 - PROGRESS: at 26.57% examples, 363355 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:11,310 : INFO : EPOCH 1 - PROGRESS: at 36.70% examples, 376745 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:12,346 : INFO : EPOCH 1 - PROGRESS: at 45.56% examples, 371275 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:13,355 : INFO : EPOCH 1 - PROGRESS: at 55.73% examples, 378143 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:14,352 : INFO : EPOCH 1 - PROGRESS: at 64.55% examples, 377290 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:15,358 : INFO : EPOCH 1 - PROGRESS: at 74.78% examples, 382260 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:16,367 : INFO : EPOCH 1 - PROGRESS: at 83.67% examples, 380281 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:17,386 : INFO : EPOCH 1 - PROGRESS: at 92.32% examples, 377928 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:18,310 : INFO : EPOCH 1: training on 5713167 raw words (4164986 effective words) took 11.1s, 375995 effective words/s\n",
      "2024-02-08 00:15:19,346 : INFO : EPOCH 2 - PROGRESS: at 9.84% examples, 402363 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:20,357 : INFO : EPOCH 2 - PROGRESS: at 18.70% examples, 381114 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:21,352 : INFO : EPOCH 2 - PROGRESS: at 28.54% examples, 391430 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:22,382 : INFO : EPOCH 2 - PROGRESS: at 37.89% examples, 389732 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:23,378 : INFO : EPOCH 2 - PROGRESS: at 46.85% examples, 384837 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:24,396 : INFO : EPOCH 2 - PROGRESS: at 55.73% examples, 379962 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:25,416 : INFO : EPOCH 2 - PROGRESS: at 64.18% examples, 376411 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:26,429 : INFO : EPOCH 2 - PROGRESS: at 73.76% examples, 378018 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:27,440 : INFO : EPOCH 2 - PROGRESS: at 84.14% examples, 382824 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:28,436 : INFO : EPOCH 2 - PROGRESS: at 92.82% examples, 381218 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:29,180 : INFO : EPOCH 2: training on 5713167 raw words (4164996 effective words) took 10.9s, 383141 effective words/s\n",
      "2024-02-08 00:15:30,208 : INFO : EPOCH 3 - PROGRESS: at 9.84% examples, 405800 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:31,244 : INFO : EPOCH 3 - PROGRESS: at 18.70% examples, 378215 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:32,263 : INFO : EPOCH 3 - PROGRESS: at 28.54% examples, 387297 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:33,266 : INFO : EPOCH 3 - PROGRESS: at 37.19% examples, 380966 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:34,269 : INFO : EPOCH 3 - PROGRESS: at 47.51% examples, 389038 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:35,278 : INFO : EPOCH 3 - PROGRESS: at 56.39% examples, 384027 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:36,298 : INFO : EPOCH 3 - PROGRESS: at 65.09% examples, 381092 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:37,318 : INFO : EPOCH 3 - PROGRESS: at 73.99% examples, 378804 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:38,321 : INFO : EPOCH 3 - PROGRESS: at 84.14% examples, 382905 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:39,317 : INFO : EPOCH 3 - PROGRESS: at 92.48% examples, 379785 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:40,118 : INFO : EPOCH 3: training on 5713167 raw words (4165158 effective words) took 10.9s, 381006 effective words/s\n",
      "2024-02-08 00:15:41,129 : INFO : EPOCH 4 - PROGRESS: at 9.66% examples, 406323 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:42,131 : INFO : EPOCH 4 - PROGRESS: at 18.19% examples, 376930 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:43,135 : INFO : EPOCH 4 - PROGRESS: at 27.92% examples, 386149 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:44,142 : INFO : EPOCH 4 - PROGRESS: at 37.56% examples, 390248 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:45,159 : INFO : EPOCH 4 - PROGRESS: at 46.46% examples, 384530 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:46,175 : INFO : EPOCH 4 - PROGRESS: at 56.94% examples, 390789 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-08 00:15:47,192 : INFO : EPOCH 4 - PROGRESS: at 65.42% examples, 385191 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:48,193 : INFO : EPOCH 4 - PROGRESS: at 74.94% examples, 385735 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:49,190 : INFO : EPOCH 4 - PROGRESS: at 85.03% examples, 389203 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:50,226 : INFO : EPOCH 4 - PROGRESS: at 95.08% examples, 391643 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-08 00:15:50,774 : INFO : EPOCH 4: training on 5713167 raw words (4163992 effective words) took 10.6s, 391008 effective words/s\n",
      "2024-02-08 00:15:50,774 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20824541 effective words) took 54.7s, 380876 effective words/s', 'datetime': '2024-02-08T00:15:50.774951', 'gensim': '4.3.2', 'python': '3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2024-02-08 00:15:50,774 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-08T00:15:50.774951', 'gensim': '4.3.2', 'python': '3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in data]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 00:15:58,055 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'W2v-movies.dat', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-02-08T00:15:58.055156', 'gensim': '4.3.2', 'python': '3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'saving'}\n",
      "2024-02-08 00:15:58,055 : INFO : not storing attribute cum_table\n",
      "2024-02-08 00:15:58,183 : INFO : saved W2v-movies.dat\n"
     ]
    }
   ],
   "source": [
    "# Worth it to save the previous embedding\n",
    "w2v.save(\"W2v-movies.dat\")\n",
    "# You will be able to reload them:\n",
    "# w2v = gensim.models.Word2Vec.load(\"W2v-movies.dat\")\n",
    "# and you can continue the learning process if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.76491594\n",
      "great and bad: 0.4745189\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.9290799498558044),\n",
       " ('\"movie\"', 0.8077793121337891),\n",
       " ('flick', 0.7757822871208191),\n",
       " ('movie,', 0.7666845321655273),\n",
       " ('dreck', 0.7309340834617615)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "w2v.wv.most_similar(\"movie\",topn=5) # 5 most similar words\n",
    "#w2v.wv.most_similar(\"awesome\",topn=5)\n",
    "#w2v.wv.most_similar(\"actor\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.7153176665306091),\n",
       " ('\"movie\"', 0.6472780108451843),\n",
       " ('pile', 0.6349601745605469)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "w2v.wv.most_similar(positive=[\"ugly\",\"movie\"],negative=[\"handsome\"],topn=3)  \n",
    "\n",
    "#w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?\n",
    "\n",
    "\n",
    "# Try other things like plurals for exemple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities.**\n",
    "\n",
    "**You can download the dataset [here](https://thome.isir.upmc.fr/classes/RITAL/questions-words.txt).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 09:54:41,273 : INFO : Evaluating word analogies for top 300000 words in the model on questions-words.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 09:54:41,568 : INFO : capital-common-countries: 4.4% (4/90)\n",
      "2024-02-07 09:54:41,772 : INFO : capital-world: 0.0% (0/71)\n",
      "2024-02-07 09:54:41,849 : INFO : currency: 0.0% (0/28)\n",
      "2024-02-07 09:54:42,582 : INFO : city-in-state: 0.0% (0/329)\n",
      "2024-02-07 09:54:43,238 : INFO : family: 34.5% (118/342)\n",
      "2024-02-07 09:54:44,950 : INFO : gram1-adjective-to-adverb: 2.0% (19/930)\n",
      "2024-02-07 09:54:45,997 : INFO : gram2-opposite: 2.2% (12/552)\n",
      "2024-02-07 09:54:48,106 : INFO : gram3-comparative: 19.1% (241/1260)\n",
      "2024-02-07 09:54:49,301 : INFO : gram4-superlative: 7.3% (51/702)\n",
      "2024-02-07 09:54:50,420 : INFO : gram5-present-participle: 17.1% (129/756)\n",
      "2024-02-07 09:54:51,515 : INFO : gram6-nationality-adjective: 2.9% (23/792)\n",
      "2024-02-07 09:54:53,233 : INFO : gram7-past-tense: 14.3% (180/1260)\n",
      "2024-02-07 09:54:54,317 : INFO : gram8-plural: 4.8% (39/812)\n",
      "2024-02-07 09:54:55,396 : INFO : gram9-plural-verbs: 28.8% (218/756)\n",
      "2024-02-07 09:54:55,399 : INFO : Quadruplets with out-of-vocabulary words: 55.6%\n",
      "2024-02-07 09:54:55,399 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2024-02-07 09:54:55,400 : INFO : Total accuracy: 11.9% (1034/8680)\n"
     ]
    }
   ],
   "source": [
    "out = w2v.wv.evaluate_word_analogies(\"questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node.\n",
    "\n",
    "**You can download the pre-trained word embedding [HERE](https://thome.isir.upmc.fr/classes/RITAL/word2vec-google-news-300.dat) .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 00:16:06,203 : INFO : loading KeyedVectors object from word2vec-google-news-300.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 00:16:09,118 : INFO : loading vectors from word2vec-google-news-300.dat.vectors.npy with mmap=None\n",
      "2024-02-08 00:16:18,156 : INFO : KeyedVectors lifecycle event {'fname': 'word2vec-google-news-300.dat', 'datetime': '2024-02-08T00:16:18.156747', 'gensim': '4.3.2', 'python': '3.11.7 (tags/v3.11.7:fa7a6f2, Dec  4 2023, 19:24:49) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "#from gensim.test.utils import get_tmpfile\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "bload = True\n",
    "fname = \"word2vec-google-news-300\"\n",
    "sdir = \"\" # Change\n",
    "\n",
    "if(bload==True):\n",
    "    wv_pre_trained = KeyedVectors.load(sdir+fname+\".dat\")\n",
    "else:    \n",
    "    wv_pre_trained = api.load(fname)\n",
    "    wv_pre_trained.save(sdir+fname+\".dat\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the \"synctactic\" and \"semantic\" evaluations again. Conclude on the pre-trained embeddings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 10:24:57,444 : INFO : Evaluating word analogies for top 300000 words in the model on questions-words.txt\n",
      "2024-02-07 10:25:09,370 : INFO : capital-common-countries: 83.2% (421/506)\n",
      "2024-02-07 10:26:20,456 : INFO : capital-world: 81.3% (3552/4368)\n",
      "2024-02-07 10:26:33,436 : INFO : currency: 28.5% (230/808)\n",
      "2024-02-07 10:27:12,555 : INFO : city-in-state: 72.1% (1779/2467)\n",
      "2024-02-07 10:27:20,487 : INFO : family: 86.2% (436/506)\n",
      "2024-02-07 10:27:37,104 : INFO : gram1-adjective-to-adverb: 29.2% (290/992)\n",
      "2024-02-07 10:27:50,769 : INFO : gram2-opposite: 43.5% (353/812)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mwv_pre_trained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_word_analogies\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestions-words.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcase_insensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#original semantic syntactic dataset.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:1371\u001b[0m, in \u001b[0;36mKeyedVectors.evaluate_word_analogies\u001b[1;34m(self, analogies, restrict_vocab, case_insensitive, dummy4unknown, similarity_function)\u001b[0m\n\u001b[0;32m   1367\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;66;03m# find the most likely prediction using 3CosAdd (vector offset) method\u001b[39;00m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;66;03m# TODO: implement 3CosMul and set-based methods for solving analogies\u001b[39;00m\n\u001b[1;32m-> 1371\u001b[0m sims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestrict_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestrict_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_to_index \u001b[38;5;241m=\u001b[39m original_key_to_index\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m sims:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gensim\\models\\keyedvectors.py:849\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indexer\u001b[38;5;241m.\u001b[39mmost_similar(mean, topn)\n\u001b[1;32m--> 849\u001b[0m dists \u001b[38;5;241m=\u001b[39m \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclip_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mclip_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms[clip_start:clip_end]\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m topn:\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dists\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = wv_pre_trained.evaluate_word_analogies(\"questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01765296  0.01079974  0.01314081  0.07143536 -0.04485846  0.01552931\n",
      "  0.02808714 -0.0593172   0.04548616  0.05103862 -0.03631634 -0.12405038\n",
      " -0.02113517  0.02423506 -0.08011907  0.07529658  0.04926082  0.0994224\n",
      " -0.03263602 -0.02624044 -0.02225281  0.05222666  0.03915137 -0.01552693\n",
      "  0.03860121 -0.01445112 -0.08239678  0.0475596   0.04513919  0.02161515\n",
      " -0.01342563  0.0030255  -0.04965216  0.00118342  0.04265644 -0.01144844\n",
      "  0.0072607   0.03666089  0.04948972  0.04719726  0.09572689 -0.03390986\n",
      "  0.06955991  0.00705163 -0.0150942  -0.01117392 -0.04584522  0.0035457\n",
      "  0.03867861  0.00056748  0.0074515   0.0395701  -0.01166216  0.00562643\n",
      "  0.00404136  0.0105268  -0.03696223 -0.04637394  0.01718493 -0.05615671\n",
      "  0.00701199  0.10361707 -0.08462278 -0.05248498 -0.03134738 -0.00143203\n",
      " -0.03514716  0.06317281  0.00251136  0.05226024  0.04034332  0.02064062\n",
      "  0.02458275  0.01235348 -0.09101634 -0.02428893  0.06711914  0.08997778\n",
      "  0.03897506  0.11832432  0.00269912 -0.06010424  0.04170331 -0.03358724\n",
      " -0.04529194 -0.03568164 -0.07898356  0.11268229  0.03160176  0.01030248\n",
      "  0.03879528  0.01964328 -0.05763498 -0.05400552 -0.04720832 -0.0574532\n",
      "  0.02996734  0.0232575   0.02345345 -0.00313512 -0.04447382 -0.03585647\n",
      "  0.02694417  0.0018761  -0.02298109 -0.01302611 -0.05168484 -0.06447897\n",
      "  0.01902528 -0.07926424 -0.07185555 -0.01640295 -0.01349872 -0.02351968\n",
      "  0.08416756 -0.00155556  0.03181882 -0.05321878  0.09698366  0.03866803\n",
      " -0.06276822 -0.00035118 -0.0521485   0.04909182 -0.05987193 -0.03836961\n",
      " -0.04733338 -0.01186131 -0.01283769  0.02473933 -0.04176967 -0.06824733\n",
      " -0.08235069 -0.00779799 -0.00231382 -0.04863885  0.04825862  0.00688838\n",
      "  0.01199677  0.04083582  0.06150005 -0.06933946  0.0332019   0.00406758\n",
      " -0.00670444  0.01473278 -0.02278099 -0.01293723 -0.06876174 -0.00688183\n",
      "  0.05721191  0.00596094 -0.06166909  0.03811067 -0.02923111 -0.02548647\n",
      " -0.04405757 -0.08017202 -0.02963999 -0.04236946  0.00438452  0.08677439\n",
      "  0.02777443  0.06551675 -0.03032794 -0.07995882  0.05827263 -0.01991563\n",
      "  0.00358681 -0.01207116 -0.09477953 -0.05481864  0.00204506 -0.09191565\n",
      " -0.01276298 -0.00345918  0.08123176 -0.08250933 -0.01845119  0.00755989\n",
      " -0.05050276 -0.02490349  0.00581709 -0.02848133 -0.03551922 -0.01844159\n",
      " -0.04354306  0.01876839  0.07674193  0.0539412   0.03621979  0.03393386\n",
      "  0.00880681  0.01417542 -0.0030089   0.02743554 -0.04370608  0.002376\n",
      " -0.04782508 -0.08091584  0.00973593  0.04462248 -0.06099541 -0.01274778\n",
      "  0.012661   -0.01235617 -0.03039814  0.00139997 -0.0044992   0.00241468\n",
      "  0.00516215  0.07141129 -0.00620272  0.00384261 -0.12126819  0.03498871\n",
      "  0.07489972  0.01696724 -0.08676462  0.00188741  0.00201656 -0.00644596\n",
      "  0.00178621 -0.02025876  0.07036694 -0.02053948  0.06190211  0.01000286\n",
      "  0.00112225  0.00133583  0.03088061 -0.05427137  0.00737122  0.02200347\n",
      "  0.06071229 -0.01121498 -0.01689853 -0.03898912  0.07370582  0.00861244\n",
      "  0.04988723  0.01315321  0.03160946 -0.05693754 -0.02411721  0.02039793\n",
      "  0.01732001  0.05484232 -0.00761445 -0.05444167  0.02195226  0.03263345\n",
      "  0.0508934   0.04945844  0.06449905 -0.01639831  0.02444822  0.00349721\n",
      " -0.07678177 -0.03884619 -0.02804799 -0.01989194 -0.05404755  0.01368234\n",
      "  0.04932885  0.14341529  0.00551281 -0.02835567 -0.0713386  -0.01791263\n",
      "  0.03514057  0.0636603   0.08501843  0.02970799  0.06343102 -0.0214994\n",
      " -0.05056563 -0.08741753 -0.04107984  0.01109391  0.01700131 -0.04336556\n",
      " -0.00939682  0.05472242  0.03688764  0.01029202 -0.09186696 -0.05118028\n",
      "  0.02827646  0.05146314 -0.0751467   0.03546817 -0.0913799   0.00484332\n",
      " -0.0445909  -0.01157486  0.01599161 -0.02677726  0.03854248 -0.01460182]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "\n",
    "def randomvec():\n",
    "    default = np.random.randn(300)\n",
    "    default = default  / np.linalg.norm(default)\n",
    "    return default\n",
    "\n",
    "def vectorize(text,mean=False):\n",
    "    \"\"\"\n",
    "    This function should vectorize one review\n",
    "\n",
    "    input: str\n",
    "    output: np.array(float)\n",
    "    \"\"\"    \n",
    "    words=word_tokenize(text.lower())\n",
    "    c=0\n",
    "    vec=np.zeros(wv_pre_trained[\"the\"].shape)\n",
    "    for word in words:\n",
    "        if word in wv_pre_trained:\n",
    "            vec+=wv_pre_trained[word]\n",
    "            c+=1\n",
    " \n",
    "    return vec/c\n",
    "    \n",
    "train,test=train_test_split(data,test_size=0.2,train_size=0.8)\n",
    "classes = [pol for text,pol in train]\n",
    "X = [vectorize(text) for text,pol in train]\n",
    "X_test = [vectorize(text) for text,pol in test]\n",
    "true = [pol for text,pol in test]\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()\n",
    "\n",
    "X_scaled=scaler.fit_transform(X)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "\n",
    "classifier=LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_scaled,classes)\n",
    "y_pred=classifier.predict(X_test_scaled)\n",
    "accuracy=accuracy_score(true,y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo** :  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
