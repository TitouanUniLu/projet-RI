{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8279abe5",
   "metadata": {},
   "source": [
    "# Word Embedding for Sequence Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879c909",
   "metadata": {},
   "source": [
    "**The goal of this practical is to use pre-trained word embedding for adressing the sequence prediction tasks studied in week 2: PoS and chunking.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdc2715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527dba92",
   "metadata": {},
   "source": [
    "## 0) Loading PoS (or chunking) datasets (small or large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1478369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    listeDoc = list()\n",
    "    with open(filename, \"r\") as f:\n",
    "        doc = list()\n",
    "        for ligne in f:\n",
    "            #print \"l : \",len(ligne),\" \",ligne\n",
    "            if len(ligne) < 2: # fin de doc\n",
    "                listeDoc.append(doc)\n",
    "                doc = list()\n",
    "                continue\n",
    "            mots = ligne.replace(\"\\n\",\"\").split(\" \")\n",
    "            doc.append((mots[0],mots[2])) # mettre mots[2] à la place de mots[1] pour le chuncking\n",
    "    return listeDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0514890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8936  docs read\n",
      "2012  docs (T) read\n"
     ]
    }
   ],
   "source": [
    "bSmall = False\n",
    "\n",
    "if(bSmall==True):\n",
    "    filename = \"../tme2/conll2000/conll2000/chtrain.txt\" \n",
    "    filenameT = \"../tme2/conll2000/conll2000/chtest.txt\" \n",
    "\n",
    "else:\n",
    "    # Larger corpus .\n",
    "    filename = \"../tme2/conll2000/conll2000/train.txt\" \n",
    "    filenameT = \"../tme2/conll2000/conll2000/test.txt\" \n",
    "\n",
    "alldocs = load(filename)\n",
    "alldocsT = load(filenameT)\n",
    "\n",
    "print(len(alldocs),\" docs read\")\n",
    "print(len(alldocsT),\" docs (T) read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a10ae1",
   "metadata": {},
   "source": [
    "# 1) Word embedding for classifying each word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91fa49d",
   "metadata": {},
   "source": [
    "### Pre-trained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9688afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "bload = True\n",
    "fname = \"word2vec-google-news-300\"\n",
    "sdir = \"\" # Change\n",
    "\n",
    "if(bload==True):\n",
    "    wv_pre_trained = KeyedVectors.load(sdir+fname+\".dat\")\n",
    "else:    \n",
    "    wv_pre_trained = api.load(fname)\n",
    "    wv_pre_trained.save(sdir+fname+\".dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9dc19",
   "metadata": {},
   "source": [
    "### Some token on the dataset are missing, we will encode them with a random vector\n",
    "This is sub-optimal, but we need to do something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38abc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomvec():\n",
    "    default = np.random.randn(300)\n",
    "    default = default  / np.linalg.norm(default)\n",
    "    return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd9a228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ****** Document ****** 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wv_pre_trained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ****** Document ******\u001b[39m\u001b[38;5;124m\"\u001b[39m,cpt)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x,pos) \u001b[38;5;129;01min\u001b[39;00m d:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m (x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwv_pre_trained\u001b[49m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (x \u001b[38;5;129;01min\u001b[39;00m dictadd)):\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not in WE, adding it with random vector\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m         dictadd[x] \u001b[38;5;241m=\u001b[39m randomvec()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wv_pre_trained' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=10) # seed the randomness\n",
    "\n",
    "dictadd = dict()\n",
    "cpt=0\n",
    "for d in alldocs:\n",
    "    cpt+=1\n",
    "    print(\" ****** Document ******\",cpt)\n",
    "    for (x,pos) in d:\n",
    "        if (not (x in wv_pre_trained) and not (x in dictadd)):\n",
    "            print(x,\" not in WE, adding it with random vector\")\n",
    "            dictadd[x] = randomvec()\n",
    "            \n",
    "for d in alldocsT:\n",
    "    cpt+=1\n",
    "    print(\" ****** TEST Document ******\",cpt)\n",
    "    for (x,pos) in d:\n",
    "        if (not (x in wv_pre_trained) and not (x in dictadd)):\n",
    "            print(x,\" not in WE, adding it with random vector\")\n",
    "            dictadd[x] = randomvec()\n",
    "            #wv_pre_trained.add_vector(x,randomvec())\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94cd47",
   "metadata": {},
   "source": [
    "### Add the (key-value) 'random' word embeddings for missing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b202e361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.87200464e-04  1.60092395e-02 -3.36059928e-02  6.48825467e-02\n",
      "  4.70259460e-03  3.46915461e-02 -7.11487383e-02  1.18286544e-02\n",
      " -3.79935987e-02 -8.77709538e-02 -7.19027501e-03  2.53977217e-02\n",
      "  2.14974508e-02  1.24082128e-02 -1.23100895e-02 -7.33138546e-02\n",
      "  3.34172463e-03 -3.28111798e-02  5.44306152e-02  1.42146405e-02\n",
      "  2.09723879e-02 -6.77637476e-03 -5.07333167e-02 -1.18474744e-03\n",
      "  1.81669518e-02  4.83812466e-02 -3.42427902e-02  9.46366563e-02\n",
      "  5.89774661e-02 -9.17315017e-03 -6.52074888e-02 -1.49930696e-04\n",
      "  3.89650627e-03 -8.03749785e-02 -4.32538353e-02 -4.38350737e-02\n",
      "  1.22720627e-02  4.99978438e-02 -1.46361943e-02  1.35591654e-02\n",
      "  4.98693921e-02  5.20003475e-02  1.77748632e-02  8.11136961e-02\n",
      " -6.68863812e-03  4.47420068e-02 -1.06641511e-02 -2.59523164e-03\n",
      "  7.36436173e-02 -2.12562177e-02  3.43948156e-02  1.47454515e-02\n",
      " -4.00881022e-02  1.06461179e-02 -6.62997365e-02 -4.30645980e-02\n",
      "  4.07102779e-02 -4.26423103e-02  2.32613310e-02 -5.80925457e-02\n",
      " -1.31760293e-03 -3.71659547e-02 -1.06654502e-02 -4.02127095e-02\n",
      " -9.76917818e-02 -8.09368193e-02  8.40926766e-02 -1.10001296e-01\n",
      "  5.53413592e-02  2.96660308e-02  4.09627184e-02 -4.35889848e-02\n",
      "  7.10859671e-02  5.41235469e-02 -3.51014324e-02  5.37383668e-02\n",
      " -5.15480451e-02  5.04582673e-02  1.11857504e-01  1.36316106e-01\n",
      " -6.44681901e-02  8.86060391e-03 -7.33753592e-02  5.97016178e-02\n",
      " -6.53132051e-03  4.18463498e-02  8.03380236e-02  1.15564406e-01\n",
      " -4.15570661e-02  6.85412884e-02  9.61713940e-02 -7.58695528e-02\n",
      "  5.39370589e-02  6.25824779e-02 -1.36673981e-02  5.49257919e-02\n",
      "  2.87972428e-02 -6.57264590e-02 -1.42151862e-02 -5.46371117e-02\n",
      " -5.20468839e-02  1.26273587e-01 -2.32957080e-02  1.21458679e-01\n",
      "  9.15120095e-02 -1.14463128e-01 -1.77768487e-02 -6.10901490e-02\n",
      "  4.96453270e-02  5.10921627e-02 -8.20520595e-02 -5.65444715e-02\n",
      "  1.73559808e-03  4.04876471e-02 -2.33225152e-02  7.24945962e-02\n",
      " -5.98148850e-04  2.37375461e-02  1.08280376e-01 -6.69028163e-02\n",
      " -3.89349684e-02  3.73600125e-02  6.23498335e-02  1.18439347e-01\n",
      "  8.79799724e-02 -6.41986579e-02  3.34716425e-03 -1.14818783e-02\n",
      "  1.99304186e-02 -4.80816066e-02 -6.05930872e-02  6.03876300e-02\n",
      " -1.54731452e-01 -2.61309720e-03 -7.89908171e-02  9.65578109e-02\n",
      " -4.24873345e-02  6.31927885e-03  1.62368675e-03  1.72571167e-02\n",
      " -2.35728454e-02 -1.83085680e-01 -8.93144961e-03  2.79504675e-02\n",
      "  9.62115452e-03 -1.74430478e-02  2.11917814e-02  4.18017469e-02\n",
      "  5.65662747e-03 -6.94911405e-02  7.09252656e-02 -2.58290190e-02\n",
      " -4.76766974e-02 -5.57200797e-02  4.11508530e-02  6.97612390e-02\n",
      "  5.31612569e-03 -1.10049993e-02  6.66137785e-02  1.05387971e-01\n",
      "  4.54133973e-02  4.52814251e-02 -7.03060627e-02 -1.86791569e-02\n",
      " -6.52278066e-02 -3.23924087e-02  4.83779609e-02  5.08017885e-03\n",
      "  4.35235351e-02 -1.00956701e-01 -1.03938067e-02 -2.35483442e-02\n",
      " -2.05191579e-02 -2.22850181e-02  6.14312813e-02  1.83111504e-02\n",
      "  7.37506896e-02 -8.38071704e-02 -5.63943014e-02  3.81162874e-02\n",
      " -6.96433103e-03 -6.54551759e-02 -6.20111264e-02  2.41668019e-02\n",
      "  4.52786610e-02 -1.07987270e-01 -2.58983355e-02 -6.62443191e-02\n",
      "  7.35281855e-02  2.74857059e-02  2.02997606e-02 -4.96682636e-02\n",
      " -1.53317347e-01 -3.26564349e-02 -3.10208201e-02  1.81632414e-02\n",
      " -1.10793039e-01 -4.50997204e-02  6.40353654e-03  1.03317369e-02\n",
      " -1.55383367e-02 -8.44555255e-03 -5.05599752e-02  1.14367425e-01\n",
      " -1.02753043e-01 -1.25103733e-02 -8.60635787e-02 -3.36875133e-02\n",
      " -2.77096927e-02 -6.63851053e-02 -7.72750229e-02 -1.36504337e-01\n",
      "  1.65590018e-01 -3.56835686e-02 -1.44409121e-03 -3.51559073e-02\n",
      " -2.52354331e-02  3.19303684e-02 -1.81016570e-03  5.11798859e-02\n",
      " -7.31908008e-02 -7.41327927e-02 -4.64812713e-03  1.90519188e-02\n",
      " -1.29591040e-02 -1.31709212e-02 -4.46021110e-02 -4.59696651e-02\n",
      "  6.28631003e-03  1.33169610e-02 -8.09848830e-02 -6.81040622e-03\n",
      "  3.80283333e-02 -5.68764918e-02  1.82538312e-02  2.21720897e-02\n",
      " -1.12225134e-02 -5.94365858e-02 -7.87691213e-03 -5.72363734e-02\n",
      " -3.30734514e-02 -2.37666885e-03 -5.73120937e-02  9.88798961e-02\n",
      "  4.22370620e-02 -1.76087264e-02  1.34887889e-01 -4.90369201e-02\n",
      " -5.01260255e-03  2.49673892e-02 -5.32558151e-02  3.62621136e-02\n",
      " -1.09287854e-02  4.11181152e-03  3.35017480e-02 -5.63437156e-02\n",
      "  1.58393857e-04 -1.18198752e-01 -2.81809410e-03 -5.81101663e-02\n",
      "  4.99068573e-02  9.24661756e-02  1.65195987e-02  4.96340729e-03\n",
      "  9.79274418e-03 -7.01219812e-02  7.29842260e-02  5.34576066e-02\n",
      " -2.33621635e-02  1.32499874e-01  2.08101887e-02  1.70069292e-01\n",
      "  2.87788995e-02 -7.94443954e-03  2.62412149e-02  6.76681697e-02\n",
      " -2.58377288e-02 -4.80666105e-03  6.14606915e-03 -6.54349476e-02\n",
      " -3.58080566e-02 -9.00373049e-03  7.78441280e-02 -8.94998834e-02\n",
      "  1.16158664e-01  4.04139385e-02  6.52045161e-02  2.78388169e-02\n",
      " -1.19649675e-02  4.70504090e-02  5.45789637e-02 -7.15214312e-02\n",
      "  2.37375754e-03  1.83697138e-02  3.73684689e-02  2.56387778e-02\n",
      " -3.76583487e-02  2.64904592e-02 -7.13645145e-02  2.74759494e-02]\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "wv_pre_trained[\"random\"]=randomvec()\n",
    "print(wv_pre_trained[\"random\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cb9f7",
   "metadata": {},
   "source": [
    "### Store the train and test datasets: a word embedding for each token in the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e1f1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "wvectors = [wv_pre_trained[word] if word not in dictadd.keys() else wv_pre_trained[\"random\"] for d in alldocs for word, pos in d]\n",
    "wvectorsT = [wv_pre_trained[word] if word not in dictadd.keys() and word!=\"Good-bye\" else wv_pre_trained[\"random\"] for d in alldocsT for word, pos in d]\n",
    "\n",
    "# On s'est rendu compte que Good-bye posait problème, d'où l'exception.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be97535",
   "metadata": {},
   "source": [
    "### Check the size of your train/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9a561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211727 47377\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "print(len(wvectors),len(wvectorsT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ca328",
   "metadata": {},
   "source": [
    "### Collecting train/test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e2b7173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22  keys in the dictionary\n",
      "23  keys in the dictionary\n",
      "[ 5  6  5 15 10 20 20 20 20  5] [ 5 15 15  5 15 15 10  5 10  5]\n"
     ]
    }
   ],
   "source": [
    "# Labels train/test\n",
    "\n",
    "buf2 = [[pos for m,pos in d ] for d in alldocs]\n",
    "cles = []\n",
    "[cles.extend(b) for b in buf2]\n",
    "cles = np.unique(np.array(cles))\n",
    "cles2ind = dict(zip(cles,range(len(cles))))\n",
    "nCles = len(cles)\n",
    "print(nCles,\" keys in the dictionary\")\n",
    "labels  = np.array([cles2ind[pos] for d in alldocs for (m,pos) in d ])\n",
    "#np.array([cles2ind[pos] for (m,pos) in d for d in alldocs])\n",
    "labelsT  = np.array([cles2ind.setdefault(pos,len(cles)) for d in alldocsT for (m,pos) in d ])\n",
    "\n",
    "print(len(cles2ind),\" keys in the dictionary\")\n",
    "print(labels[:10],labelsT[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "001760be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211727,)\n",
      "(47377,)\n",
      "[ 5  6  5 ... 15  1 21]\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(labelsT.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5133930",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression Model! \n",
    "**An compare performances to the baseline and sequence models (HMM/CRF) or practical 2a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd1c94ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6914536589484349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "wvectors_scaled = scaler.fit_transform(wvectors)\n",
    "wvectorsT_scaled = scaler.transform(wvectorsT)\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "classifier.fit(wvectors_scaled, labels)\n",
    "\n",
    "pred = classifier.predict(wvectorsT_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(pred, labelsT)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbd43a",
   "metadata": {},
   "source": [
    "# 2) Using word embedding with CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6ce24",
   "metadata": {},
   "source": [
    "## We will define the following features functions for CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3668c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_wv(sentence, index):\n",
    "    v = wv_pre_trained.get_vector(sentence[index])\n",
    "    d = {'f'+str(i):v[i] for i in range(300)}\n",
    "    return d\n",
    "\n",
    "def features_structural(sentence, index):\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "     ## We will define the following features functions for CRF## We will define the following features functions for CRF   'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "def features_wv_plus_structural(sentence, index):\n",
    "    v = wv_pre_trained.get_vector(sentence[index]) \n",
    "    d = {'f'+str(i):v[i] for i in range(300)}\n",
    "\n",
    "    return {**d, **features_structural(sentence, index)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38a9f9",
   "metadata": {},
   "source": [
    "## [Question]: explain what the 3 feature functions encode and what their differences are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17efa9d2",
   "metadata": {},
   "source": [
    "Première fonction : capture l'information sémantique \n",
    "<br>Deuxième fonction : capture l'information structurelle et syntaxique\n",
    "<br>Troisème fonction : combine les deux fonctions définies auparavant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6a31f",
   "metadata": {},
   "source": [
    "### You can now train a CRF with the 3 features and analyse the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6b1e451",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1961192166.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    tagger = ## YOUR CODE HERE\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.crf import CRFTagger\n",
    "\n",
    "tagger = ## YOUR CODE HERE\n",
    "## Train the model                  \n",
    "## Evaluate performances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
